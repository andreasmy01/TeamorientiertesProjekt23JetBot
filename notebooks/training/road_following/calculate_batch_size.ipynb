{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import typing as t\n",
    "\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import ResNet18_Weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "model.fc = torch.nn.Linear(512, 2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch_size(\n",
    "        model: nn.Module,\n",
    "        device: torch.device,\n",
    "        input_shape: t.Tuple[int, int, int],\n",
    "        output_shape: t.Tuple[int],\n",
    "        dataset_size: int,\n",
    "        max_batch_size: int = None,\n",
    "        num_iterations: int = 5,\n",
    ") -> int:\n",
    "    model.to(device)\n",
    "    model.train(True)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    batch_size = 2\n",
    "    while True:\n",
    "        if max_batch_size is not None and batch_size >= max_batch_size:\n",
    "            batch_size = max_batch_size\n",
    "            break\n",
    "        if batch_size >= dataset_size:\n",
    "            batch_size = batch_size // 2\n",
    "            break\n",
    "        try:\n",
    "            for _ in range(num_iterations):\n",
    "                # dummy inputs and targets\n",
    "                inputs = torch.rand(*(batch_size, *input_shape), device=device)\n",
    "                targets = torch.rand(*(batch_size, *output_shape), device=device)\n",
    "                outputs = model(inputs)\n",
    "                loss = F.mse_loss(targets, outputs)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            batch_size *= 2\n",
    "        except RuntimeError:\n",
    "            batch_size //= 2\n",
    "            break\n",
    "    del model, optimizer\n",
    "    torch.cuda.empty_cache()\n",
    "    return batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_x(path, width):\n",
    "    \"\"\"Gets the x value from the image filename\"\"\"\n",
    "    return (float(int(path.split(\"_\")[1])) - width / 2) / (width / 2)\n",
    "\n",
    "\n",
    "def get_y(path, height):\n",
    "    \"\"\"Gets the y value from the image filename\"\"\"\n",
    "    return (float(int(path.split(\"_\")[2])) - height / 2) / (height / 2)\n",
    "\n",
    "\n",
    "class XYDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, directory, random_hflips=False):\n",
    "        self.directory = directory\n",
    "        self.random_hflips = random_hflips\n",
    "        self.image_paths = glob.glob(os.path.join(self.directory, '*.jpg'))\n",
    "        self.color_jitter = transforms.ColorJitter(0.3, 0.3, 0.3, 0.3)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "\n",
    "        image = PIL.Image.open(image_path)\n",
    "        width, height = image.size\n",
    "        x = float(get_x(os.path.basename(image_path), width))\n",
    "        y = float(get_y(os.path.basename(image_path), height))\n",
    "\n",
    "        if float(np.random.rand(1)) > 0.5:\n",
    "            image = transforms.functional.hflip(image)\n",
    "            x = -x\n",
    "\n",
    "        image = self.color_jitter(image)\n",
    "        image = transforms.functional.resize(image, (224, 224))\n",
    "        image = transforms.functional.to_tensor(image)\n",
    "        image = image.numpy()[::-1].copy()\n",
    "        image = torch.from_numpy(image)\n",
    "        image = transforms.functional.normalize(image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "        return image, torch.tensor([x, y]).float()\n",
    "\n",
    "\n",
    "dataset = XYDataset('dataset_xy', random_hflips=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = get_batch_size(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    input_shape=(3, 224, 224),\n",
    "    output_shape=(2,),\n",
    "    dataset_size=len(dataset),\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
